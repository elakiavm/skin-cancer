{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c46e2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Importing Essential Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78677416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Enhance Image with CLAHE\n",
    "def enhance_image(image_array):\n",
    "    lab = cv2.cvtColor(image_array, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    merged = cv2.merge((cl, a, b))\n",
    "    enhanced = cv2.cvtColor(merged, cv2.COLOR_LAB2RGB)\n",
    "    return enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90f7c257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2109 images.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Loading images and labels into arrays with CLAHE\n",
    "def load_images_and_labels(dataset_path, img_size=(224, 224)):\n",
    "    categories = ['benign', 'malignant']\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for category in categories:\n",
    "        folder_path = os.path.join(dataset_path, category)\n",
    "        class_label = category\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"❌ Folder not found: {folder_path}\")\n",
    "            continue\n",
    "        for img_file in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            try:\n",
    "                img = Image.open(img_path).resize(img_size, Image.LANCZOS).convert('RGB')\n",
    "                img_np = np.array(img)\n",
    "                img_enhanced = enhance_image(img_np)\n",
    "                data.append(img_enhanced)\n",
    "                labels.append(class_label)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Skipping file: {img_path} due to error: {e}\")\n",
    "                continue\n",
    "    print(f\"✅ Loaded {len(labels)} images.\")\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Step 4: Categorical Labels\n",
    "data_path = r\"C:\\Users\\LLR User\\Desktop\\Coding\\code\\skin-cancer\\Dataset\"  # Full path\n",
    "\n",
    "data, labels = load_images_and_labels(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "435b86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "if len(labels) == 0:\n",
    "    raise ValueError(\"No image data found. Check your dataset path and folder names.\")\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded, num_classes=len(np.unique(labels_encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51de6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Normalization using dataset mean and std\n",
    "mean = np.mean(data, axis=(0, 1, 2), keepdims=True, dtype=np.float32)\n",
    "std = np.std(data, axis=(0, 1, 2), keepdims=True, dtype=np.float32)\n",
    "std[std == 0] = 1e-6  # Avoid division by zero\n",
    "data = (data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9febc5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train and Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels_categorical, test_size=0.2, random_state=42, stratify=labels_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a07076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "val_datagen = ImageDataGenerator()\n",
    "train_generator = train_datagen.flow(X_train[:512], y_train[:512], batch_size=32)\n",
    "val_generator = val_datagen.flow(X_test[:128], y_test[:128], batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4401000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape=(224, 224, 3)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34895206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LLR User\\miniconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 723ms/step - accuracy: 0.8032 - loss: 0.4544 - val_accuracy: 0.7969 - val_loss: 0.3834 - learning_rate: 5.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 442ms/step - accuracy: 0.8154 - loss: 0.4058 - val_accuracy: 0.8281 - val_loss: 0.3744 - learning_rate: 5.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 495ms/step - accuracy: 0.8528 - loss: 0.3562 - val_accuracy: 0.8125 - val_loss: 0.3881 - learning_rate: 5.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 468ms/step - accuracy: 0.8385 - loss: 0.3850 - val_accuracy: 0.7969 - val_loss: 0.3628 - learning_rate: 5.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 464ms/step - accuracy: 0.8395 - loss: 0.3156 - val_accuracy: 0.8281 - val_loss: 0.3700 - learning_rate: 5.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 481ms/step - accuracy: 0.8329 - loss: 0.3957 - val_accuracy: 0.8281 - val_loss: 0.3515 - learning_rate: 5.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - accuracy: 0.8115 - loss: 0.3868\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 463ms/step - accuracy: 0.8136 - loss: 0.3856 - val_accuracy: 0.8281 - val_loss: 0.3541 - learning_rate: 5.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 462ms/step - accuracy: 0.8340 - loss: 0.3693 - val_accuracy: 0.8281 - val_loss: 0.3478 - learning_rate: 2.5000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 469ms/step - accuracy: 0.8064 - loss: 0.3815 - val_accuracy: 0.8281 - val_loss: 0.3459 - learning_rate: 2.5000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 476ms/step - accuracy: 0.8833 - loss: 0.3133 - val_accuracy: 0.8125 - val_loss: 0.3479 - learning_rate: 2.5000e-05\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Train the model on smaller batches to reduce memory usage\n",
    "X_train_small = X_train[:256]\n",
    "y_train_small = y_train[:256]\n",
    "X_test_small = X_test[:64]\n",
    "y_test_small = y_test[:64]\n",
    "\n",
    "train_generator = train_datagen.flow(X_train_small, y_train_small, batch_size=16)\n",
    "val_generator = val_datagen.flow(X_test_small, y_test_small, batch_size=16)\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.5, min_lr=1e-7)\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=10, callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d254f10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 130ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0dc23b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7867298578199052\n",
      "Confusion Matrix:\n",
      " [[176  55]\n",
      " [ 35 156]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.80       231\n",
      "           1       0.74      0.82      0.78       191\n",
      "\n",
      "    accuracy                           0.79       422\n",
      "   macro avg       0.79      0.79      0.79       422\n",
      "weighted avg       0.79      0.79      0.79       422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_true, y_pred_classes)\n",
    "print(\"Test Accuracy:\", acc)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226f6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
